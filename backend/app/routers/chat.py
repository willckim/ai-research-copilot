import logging
from fastapi import APIRouter, Depends, HTTPException, Body
from pydantic import BaseModel, Field
from sqlalchemy.orm import Session
from sqlalchemy import text

from ..db import SessionLocal
from ..rag import Embedder, call_llm          # ✅ updated import
from ..deps import settings

logger = logging.getLogger(__name__)
router = APIRouter(prefix="/chat", tags=["chat"])


def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()


# ✅ provider-aware: reads EMBED_PROVIDER/EMBED_MODEL from env
embedder = Embedder()


# ---------- OpenAPI models ----------

class ChatReq(BaseModel):
    question: str = Field(..., description="User's question to answer using retrieved document chunks.")
    top_k: int = Field(5, ge=1, le=50, description="How many nearest chunks to retrieve for context.")
    doc_id: str | None = Field(
        None,
        description="Optional document ID to restrict retrieval to a single ingested document."
    )
    verbose: bool = Field(
        False,
        description="If true, returns the full retrieved context in the response for debugging/inspection."
    )

    model_config = {
        "json_schema_extra": {
            "examples": [
                {
                    "summary": "Basic query across all documents",
                    "value": {"question": "Summarize the report in 2 sentences", "top_k": 5},
                },
                {
                    "summary": "Query a specific document with verbose context",
                    "value": {
                        "question": "What risks and safeguards are mentioned?",
                        "top_k": 5,
                        "doc_id": "healthcare",
                        "verbose": True,
                    },
                },
            ]
        }
    }


class ChatResp(BaseModel):
    answer: str = Field(..., description="Final answer generated by the LLM based on retrieved context.")
    citations: list[str] = Field(
        ..., description="List of doc_id:chunk_id pairs corresponding to the chunks used in context."
    )
    context: str | None = Field(
        None, description="Full retrieved context block (only present when verbose=true)."
    )


def _to_pgvector_literal(vec) -> str:
    return "[" + ",".join(f"{float(x):.6f}" for x in vec) + "]"


PREVIEW_CHARS = 600
MAX_CTX_CHARS = 4000


@router.post(
    "",
    summary="Chat",
    response_model=ChatResp,
    responses={
        502: {"description": "Bad Gateway — LLM connection/HTTP error"},
        504: {"description": "Gateway Timeout — LLM did not respond in time"},
    },
)
async def chat(
    req: ChatReq = Body(
        ...,
        description="Chat request including question, retrieval options, and optional verbose flag.",
    ),
    db: Session = Depends(get_db),
):
    """
    Answer a question using Retrieval-Augmented Generation (RAG).
    """
    # 1) Embed the question
    qvec = embedder.embed([req.question])[0]

    # Optional: catch dim mismatches early (e.g., 384 local vs 1536 prod)
    if len(qvec) != settings.EMBED_DIM:
        raise HTTPException(
            status_code=500,
            detail=f"Embedding dim mismatch: got {len(qvec)}, expected {settings.EMBED_DIM}. "
                   f"Check EMBED_MODEL/EMBED_DIM and the DB column vector({settings.EMBED_DIM})."
        )

    qvec_lit = _to_pgvector_literal(qvec)

    # 2) Retrieval SQL
    base_sql = f"""
        SELECT doc_id, chunk_id, content,
               (embedding <=> '{qvec_lit}'::vector) AS dist
        FROM documents
        {{where_clause}}
        ORDER BY dist
        LIMIT :k
    """
    params = {"k": req.top_k}
    where_clause = ""
    if req.doc_id:
        where_clause = "WHERE doc_id = :doc_id"
        params["doc_id"] = req.doc_id

    sql = text(base_sql.replace("{where_clause}", where_clause))
    rows = db.execute(sql, params).fetchall()

    # 2.5) Logs
    if rows:
        previews = [
            {
                "doc_id": r[0],
                "chunk_id": int(r[1]),
                "dist": float(r[3]),
                "preview": (r[2] or "")[:PREVIEW_CHARS].replace("\n", " "),
            }
            for r in rows
        ]
        logger.info(
            "RAG retrieval",
            extra={"question": req.question, "doc_id": req.doc_id, "top_k": req.top_k, "hits": previews},
        )
        print(
            "RAG TOP HITS:",
            "; ".join([f"{p['doc_id']}#{p['chunk_id']} d={p['dist']:.4f} '{p['preview']}'" for p in previews]),
        )
    else:
        logger.warning(
            "RAG retrieval returned no rows",
            extra={"question": req.question, "doc_id": req.doc_id, "top_k": req.top_k},
        )
        return ChatResp(
            answer="I couldn't find relevant context yet. Try ingesting a document first.",
            citations=[],
        )

    # 3) Build context
    ctx_block_full = "\n\n".join([f"[{r[0]} chunk {r[1]}] {r[2]}" for r in rows])

    # Guardrail for LLM input length
    ctx_for_llm = ctx_block_full
    if len(ctx_for_llm) > MAX_CTX_CHARS:
        ctx_for_llm = ctx_for_llm[:MAX_CTX_CHARS] + "\n...[truncated]..."

    # 4) LLM call
    system = (
        "You are a helpful assistant. Use ONLY the provided context. "
        "If the answer isn't in the context, say you don't know. Cite chunk numbers."
    )
    prompt = f"{system}\n\nContext:\n{ctx_for_llm}\n\nQuestion: {req.question}\nAnswer:"

    try:
        answer = await call_llm(prompt)     # ✅ provider-aware call
    except Exception as e:
        raise HTTPException(status_code=502, detail=f"[LLM unexpected error] {e}")

    if answer.startswith("[LLM timeout]"):
        raise HTTPException(status_code=504, detail=answer)
    if answer.startswith("[LLM connection error]") or answer.startswith("[LLM error]"):
        raise HTTPException(status_code=502, detail=answer)

    # 5) Response
    return ChatResp(
        answer=answer,
        citations=[f"{r[0]}:{r[1]}" for r in rows],
        context=ctx_block_full if req.verbose else None,
    )
